{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation import visualise_features, visualise_dataset\n",
    "from utilities import load_dataframe, performance, cross_validation_feature_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel if needed\n",
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "### Visualise our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our dataframe\n",
    "df = load_dataframe()\n",
    "visualise_dataset(df) # Visualised for a single year\n",
    "print(df[\"northBound\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't tell us a huge amount. Let's split the volume directions up into separate dataframes and have a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate data frames, one for each junction\n",
    "df_north = df.drop(columns=[\"southBound\"])\n",
    "df_south = df.drop(columns=[\"northBound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataframe, let's look at a seasonality and trend plot. This could illuminate some more details to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base features performance, where we use K Nearest Neighbors.\n",
    "# Here is our baseline, now we add features.\n",
    "performance(df_north, \"northBound\")\n",
    "visualise_features([\"dayOfWeek\", \"month\", \"time\"], df_north, f\"Base Features - Northbound\", \"northBound\")\n",
    "\n",
    "performance(df_south, \"southBound\")\n",
    "visualise_features([\"dayOfWeek\", \"month\", \"time\"], df_south, f\"Base Features - Southbound\", \"southBound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see:\n",
    "1. A weekly seasonality\n",
    "2. A yearly seasonality\n",
    "3. No overall trend throughout the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_north[\"quarter\"] = df.index.quarter\n",
    "df_north[\"weekOfYear\"] = df.index.weekofyear\n",
    "df_north[\"dayOfYear\"] = df.index.dayofyear\n",
    "\n",
    "df_south[\"quarter\"] = df.index.quarter\n",
    "df_south[\"weekOfYear\"] = df.index.weekofyear\n",
    "df_south[\"dayOfYear\"] = df.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(df_north, \"northBound\")\n",
    "# visualise_features([\"quarter\", \"weekOfYear\", \"dayOfYear\"], df_north, \"Additional Features\", \"northBound\")\n",
    "\n",
    "\n",
    "performance(df_south, \"southBound\")\n",
    "# visualise_features([\"quarter\", \"weekOfYear\", \"dayOfYear\"], df_south, \"Additional Features\", \"southBound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some more date-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our dataframes back to original features by running cell towards top\n",
    "df_north = df.drop(columns=[\"southBound\"])\n",
    "df_south = df.drop(columns=[\"northBound\"])\n",
    "\n",
    "import pandas as pd\n",
    "ireland_holidays = {(1,1), (17,3), (5,4), (3,5), (7,6), (2,8), (25,10), (25,12), (26,12)}\n",
    "# Get our dataframes back to original features\n",
    "df_north = df.drop(columns=[\"southBound\"])\n",
    "df_south = df.drop(columns=[\"northBound\"])\n",
    "\n",
    "# Define a function to use in our mapping\n",
    "def f(x):\n",
    "    day = x.day\n",
    "    month = x.month\n",
    "    if((day,month) in ireland_holidays):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_north[\"weekday\"] = df.index.weekday\n",
    "series = pd.Series(df_north.index).apply(f)\n",
    "df_north[\"holiday\"] = series.to_list()\n",
    "df_north = df_north.fillna(0)\n",
    "\n",
    "df_south[\"weekday\"] = df.index.weekday\n",
    "series = pd.Series(df_south.index).apply(f)\n",
    "df_south[\"holiday\"] = series.to_list()\n",
    "df_south = df_south.fillna(0)\n",
    "\n",
    "\n",
    "# Check performance\n",
    "performance(df_north, \"northBound\")\n",
    "performance(df_south, \"southBound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Normalise\n",
    "north_volumes = df_north[\"northBound\"]\n",
    "south_volumes = df_south[\"southBound\"]\n",
    "\n",
    "# Shape series\n",
    "north_volumes = north_volumes.values.reshape(len(north_volumes), 1)\n",
    "south_volumes = south_volumes.values.reshape(len(south_volumes), 1)\n",
    "\n",
    "# train and run min-max scaler\n",
    "scaler_north = MinMaxScaler(feature_range=(0,1)).fit(north_volumes)\n",
    "scaler_south = MinMaxScaler(feature_range=(0,1)).fit(south_volumes)\n",
    "\n",
    "# Normalise\n",
    "normalised_north = scaler_north.transform(north_volumes)\n",
    "normalised_south = scaler_south.transform(south_volumes)\n",
    "\n",
    "# Set columns\n",
    "df_north[\"northBound\"] = normalised_north.flatten().tolist()\n",
    "df_south[\"southBound\"] = normalised_south.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_north"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review:\n",
    "Looks like our base features and final two features do best.\n",
    "Newer features perform quite poorly, though this could be because we have such a small dataset right now.#\n",
    "However, the dayOfYear plot demonstrates some pretty significant seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Create lagging features\n",
    "#### Need to encode components of time series data such as seasonality, trend and cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 20, 5\n",
    "fig = plot_acf(df_north[\"northBound\"], lags=24)\n",
    "\n",
    "plt.title(\"Autocorrelation of Traffic Volume for Northbound traffic (Through a day)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.xlabel(\"Lag at k\")\n",
    "fig.show()\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 20, 5\n",
    "fig = plot_acf(df_south[\"southBound\"], lags=24)\n",
    "\n",
    "plt.title(\"Autocorrelation of Traffic Volume for Southbound traffic (Through a day)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.xlabel(\"Lag at k\")\n",
    "fig.show()\n",
    "\n",
    "# rcParams[\"figure.figsize\"] = 20, 5\n",
    "# fig = plot_acf(df_north[\"northBound\"], lags=24)\n",
    "\n",
    "# plt.title(\"Autocorrelation of Traffic Volume for Northbound traffic (Through a day)\")\n",
    "# plt.ylabel(\"Correlation\")\n",
    "# plt.xlabel(\"Lag at k\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_north.copy()\n",
    "cross_validation_feature_params(test_params=[1,3,5,10,11,12,24], df=temp, feature_type=\"lag\", target_var=\"northBound\")\n",
    "\n",
    "temp_south = df_south.copy()\n",
    "cross_validation_feature_params(test_params=[1,3,5,10,11,12,24], df=temp_south, feature_type=\"lag\", target_var=\"southBound\")\n",
    "\n",
    "# Hour\n",
    "temp[\"volume_lag_1\"] = temp[\"northBound\"].shift(1, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "temp[\"volume_lag_2\"] = temp[\"northBound\"].shift(2, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "temp[\"volume_lag_3\"] = temp[\"northBound\"].shift(3, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "temp[\"volume_lag_4\"] = temp[\"northBound\"].shift(4, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "temp[\"volume_lag_5\"] = temp[\"northBound\"].shift(5, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "temp[\"volume_lag_6\"] = temp[\"northBound\"].shift(6, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "# Difference features\n",
    "temp[\"volume_lag_1_diff\"] = temp[\"volume_lag_1\"] - temp[\"northBound\"].shift(2, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "# 12 hours\n",
    "temp[\"volume_lag_halfday\"] = temp[\"northBound\"].shift(12, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "# Day\n",
    "temp[\"volume_lag_24\"] = temp[\"northBound\"].shift(24, fill_value=0)\n",
    "performance(temp, \"northBound\")\n",
    "# # Week\n",
    "# temp[\"volume_lag_week\"] = temp[\"northBound\"].shift(168, fill_value=0)\n",
    "# performance(temp, \"northBound\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set lags, and a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_north = df.drop(columns=[\"southBound\"])\n",
    "df_north[\"volume_lag_1\"] = df_north[\"northBound\"].shift(1, fill_value=0)\n",
    "df_north[\"volume_lag_2\"] = df_north[\"northBound\"].shift(2, fill_value=0)\n",
    "df_north[\"volume_lag_3\"] = df_north[\"northBound\"].shift(3, fill_value=0)\n",
    "df_north[\"volume_lag_4\"] = df_north[\"northBound\"].shift(4, fill_value=0)\n",
    "df_north[\"volume_lag_5\"] = df_north[\"northBound\"].shift(5, fill_value=0)\n",
    "df_north[\"volume_lag_6\"] = df_north[\"northBound\"].shift(6, fill_value=0)\n",
    "df_north[\"volume_lag_12\"] = df_north[\"northBound\"].shift(12, fill_value=0)\n",
    "df_north[\"volume_lag_1_diff\"] = df_north[\"volume_lag_1\"] - df_north[\"northBound\"].shift(2, fill_value=0)\n",
    "performance(df_north, \"northBound\")\n",
    "\n",
    "df_south = df.drop(columns=[\"northBound\"])\n",
    "df_south[\"volume_lag_1\"] = df_south[\"southBound\"].shift(1, fill_value=0)\n",
    "df_south[\"volume_lag_2\"] = df_south[\"southBound\"].shift(2, fill_value=0)\n",
    "df_south[\"volume_lag_3\"] = df_south[\"southBound\"].shift(3, fill_value=0)\n",
    "df_south[\"volume_lag_4\"] = df_south[\"southBound\"].shift(4, fill_value=0)\n",
    "df_south[\"volume_lag_5\"] = df_south[\"southBound\"].shift(5, fill_value=0)\n",
    "df_south[\"volume_lag_6\"] = df_south[\"southBound\"].shift(6, fill_value=0)\n",
    "df_south[\"volume_lag_12\"] = df_south[\"southBound\"].shift(12, fill_value=0)\n",
    "df_south[\"volume_lag_1_diff\"] = df_south[\"volume_lag_1\"] - df_south[\"southBound\"].shift(2, fill_value=0)\n",
    "performance(df_south, \"southBound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that the southbound performance doesnt improve to the same extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling window stuff is more or less ignored, I think we're alright with just lags for this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_north.copy()\n",
    "cross_validation_feature_params(test_params=[2,3,5,7,9,11,13,24], df=temp, feature_type=\"rolling_window_max\", target_var=\"northBound\")\n",
    "temp['window_max'] = temp['northBound'].rolling(window = 2).max()\n",
    "temp['window_max'] = temp['window_max'].fillna(0)\n",
    "performance(temp, \"northBound\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower error again with the rolling window. Try mean rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_a = df_north.copy()\n",
    "cross_validation_feature_params(test_params=[2,3,5,7,9,11,13], df=temp, feature_type=\"rolling_window_mean\", target_var=\"northBound\")\n",
    "temp['window_mean'] = temp['northBound'].rolling(window = 2).mean()\n",
    "temp['window_mean'] = temp['window_mean'].fillna(0)\n",
    "performance(temp, \"northBound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better again! See if min does any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_north.copy()\n",
    "cross_validation_feature_params(test_params=[2,3,5,7,9,11,13], df=temp, feature_type=\"rolling_window_min\", target_var=\"northBound\")\n",
    "temp['window_min'] = temp['northBound'].rolling(window = 2).min()\n",
    "temp['window_min'] = temp['window_min'].fillna(0)\n",
    "performance(temp, \"northBound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_north"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the mean rolling window as a feature. These will be added once we've figured out how to use em for forecasting. They do a pretty nice job though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Northbound\n",
    "# df_north['window_mean'] = df_north['northBound'].rolling(window = 2).mean()\n",
    "# df_north['window_mean'] = df_north['window_mean'].fillna(0)\n",
    "# performance(df_north, \"northBound\")\n",
    "# # Southbound\n",
    "# df_south['window_mean'] = df_south['southBound'].rolling(window = 2).mean()\n",
    "# df_south['window_mean'] = df_south['window_mean'].fillna(0)\n",
    "# performance(df_south, \"southBound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see better performance as we dont have the cross validated features also in the dataframe. \n",
    "This performance is particularly good for such a basic model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We're going to try a few different models provided by sklearn. We could look beyond into things like Keras, ARIMA and XGBOOST, but I don't think it's necessary for this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold out a validation set. This will be used later once we've narrowed down the params for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to hold out 12 hours of data points to predict on!~\n",
    "from utilities import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forecast size\n",
    "n_forecast = 24*30\n",
    "lag_range = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 12-previous points\n",
    "lagged_points = df_north.to_numpy()[-12-lag_range-1:-12]  # This gives us last twelve rows of training data\n",
    "lagged_points = lagged_points[\n",
    "    :, 3\n",
    "]  # Get the volume for each row, these are our lagged points\n",
    "\n",
    "\n",
    "# Target Variable\n",
    "y_north = df_north[\"northBound\"].to_numpy()\n",
    "y_south = df_south[\"southBound\"].to_numpy()\n",
    "\n",
    "# Feature Vectors\n",
    "X_north = df_north.drop(columns=[\"northBound\"]).to_numpy()\n",
    "X_south = df_south.drop(columns=[\"southBound\"]).to_numpy()\n",
    "\n",
    "\n",
    "# Hold out a validation set\n",
    "(X_north, X_north_val, y_north, y_north_val) = train_test_split(X_north, y_north, test_size=n_forecast)\n",
    "(X_south, X_south_val, y_south, y_south_val) = train_test_split(X_south, y_south, test_size=n_forecast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Timeseries Cross Validation\n",
    "# Visualise a time-series split of 5 (80/20)\n",
    "# ts_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# for i, (train, test) in enumerate(ts_cv.split(X_north, y_north)):\n",
    "#     p1 = ax.scatter(train, [i] * len(train), c='blue', marker=\"_\", lw=8)\n",
    "#     p2 = ax.scatter(test, [i] * len(test), c='red', marker=\"_\", lw=8)\n",
    "#     ax.set(\n",
    "#         title=\"Timeseries Split\",\n",
    "#         xlabel=\"Data point index\",\n",
    "#         ylabel=\"Cross Validation Iteration\",\n",
    "#         ylim=[5, -1],\n",
    "#     )\n",
    "#     ax.legend([p1, p2], [\"Training\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's look into Lasso and Ridge Regression first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluate_lasso_hyperparams, evaluate_ridge_hyperparams\n",
    "evaluate_lasso_hyperparams(X_north,y_north, [3,50,100,200,300])\n",
    "evaluate_ridge_hyperparams(X_north,y_north, [0.000001,0.0001,0.1,10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty awful performance lol. Not really surprising though. What is surprising is how well kNN does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluate_decision_tree_hyperparams\n",
    "decision_tree_model=evaluate_decision_tree_hyperparams(X_north,y_north, [i for i in range(1,12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Max Depth of 6 looks good here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluate_knn_k, evaluate_knn_k\n",
    "evaluate_knn_k(X_north,y_north, [5,10,20,30,50,100,150,200])\n",
    "# Not using gaussian weighting, seems to cause issues.\n",
    "# 10 does best. This is interesting! Usually for large datasets, largers k's are better.\n",
    "# For time series data though it makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try build on our decision tree using AdaBoost\n",
    "Tuning parameters takes a while so I'll leave it commented out. DONT RUN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes ages\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# from models import evaluate_ada_boost_hyperparams\n",
    "# from utilities import get_decision_tree_models\n",
    "\n",
    "# params_grid={\n",
    "#     \"base_estimator\": [model for model in get_decision_tree_models(max_depth_array=range(1,20))],\n",
    "#     \"n_estimators\":[i for i in range(5,150,5)],\n",
    "#     \"learning_rate\":[0.01,0.1,1,1.5,2,5,10],\n",
    "#     \"loss\":['linear', 'square', 'exponential']\n",
    "# }\n",
    "\n",
    "# evaluate_ada_boost_hyperparams(X_north,y_north,params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# import numpy as np\n",
    "ada_boost_model = AdaBoostRegressor(\n",
    "     base_estimator=DecisionTreeRegressor(max_depth=16),\n",
    "     n_estimators=135,\n",
    "     learning_rate=1,\n",
    "     loss='linear')\n",
    "\n",
    "# ada_boost_model.fit(X_north,y_north)\n",
    "# cv = TimeSeriesSplit(n_splits=5)\n",
    "# scores = cross_validate(\n",
    "#         ada_boost_model,\n",
    "#         X_north,\n",
    "#         y_north,\n",
    "#         cv=cv,\n",
    "#         scoring=\"neg_mean_squared_error\",\n",
    "#         return_estimator=True,\n",
    "#     )\n",
    "\n",
    "# base_mse = np.sqrt(-np.mean(scores[\"test_score\"]))\n",
    "# print(f\"Ada Boost Performance MSE is: {format(base_mse)}\")\n",
    "\n",
    "# Last recoreded performance was 233.62 MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit better than the original decision tree.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Grid search to find the best combination of params for a random forest.\n",
    "With those best params, we then test the performance. Takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import evaluate_random_forest_hyperparams\n",
    "# test_params = {\n",
    "# \"n_estimators\": [20, 50, 60, 100],\n",
    "# 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "# 'max_depth' : [i for i in range(5,20)]\n",
    "# }\n",
    "# evaluate_random_forest_hyperparams(X_north, y_north, test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a figure for the performance\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# cv = TimeSeriesSplit(n_splits=5)\n",
    "#rf_model = RandomForestRegressor(n_estimators=60, max_features=\"auto\",max_depth=14)\n",
    "# scores = cross_validate(\n",
    "#         rf_model,\n",
    "#         X_north,\n",
    "#         y_north,\n",
    "#         cv=cv,\n",
    "#         scoring=\"neg_mean_squared_error\",\n",
    "#         return_estimator=True,\n",
    "#     )\n",
    "\n",
    "# base_mse = np.sqrt(-np.mean(scores[\"test_score\"]))\n",
    "# print(f\"MSE is: {format(base_mse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the best hyper params for an ANN. We'll plot the error bars for it since the slides do it. Takes about six minutes to run... Comment it out then run it once everything else is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import evaluate_MLP_hidden_nodes, evaluate_MLP_penalty_weight\n",
    "# evaluate_MLP_hidden_nodes(X_north,y_north, [5,10,15,30,50,100])\n",
    "# evaluate_MLP_penalty_weight(X_north,y_north, [0.1,1,10,100,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a baseline performance first.\n",
    "We'll use a Dummy Regressor, which predicts the mean volume for every timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dummy_mean = DummyRegressor(strategy=\"mean\").fit(X_north, y_north)\n",
    "ydummy_mean = dummy_mean.predict(X_north_val)\n",
    "\n",
    "print(mean_squared_error(y_north_val, ydummy_mean))\n",
    "\n",
    "from visualisation import visualise_forecast_vs_true\n",
    "x_axis = df_north.iloc[-n_forecast:].index.to_numpy()\n",
    "#visualise_forecast_vs_true(x_axis, y_north_val, ydummy_mean, model_name=\"K-Nearest Neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from forecasting import n_one_step_ahead_prediction\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=10, weights=\"distance\").fit(X_north, y_north)\n",
    "y_forecast_north = n_one_step_ahead_prediction(knn_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(mean_squared_error(y_north_val, y_forecast_north))\n",
    "\n",
    "# Visualise\n",
    "from visualisation import visualise_forecast_vs_true\n",
    "x_axis = df_north.iloc[-n_forecast:].index.to_numpy()\n",
    "visualise_forecast_vs_true(x_axis, y_north_val, y_forecast_north, model_name=\"K-Nearest Neighbors\", baseline_y=ydummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#y_forecast_north = n_one_step_ahead_prediction(decision_tree_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(\"MSE: \"+str(mean_squared_error(y_north_val, y_forecast_north)))\n",
    "#x_axis = df.iloc[-n_forecast:].index.to_numpy()\n",
    "#visualise_forecast_vs_true(x_axis,y_north_val,y_forecast_north,\"Decision Tree\", baseline_y=ydummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_boost_model = AdaBoostRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(max_depth=17),\n",
    "    n_estimators=40,\n",
    "    learning_rate=1,\n",
    "    loss='linear').fit(X_north,y_north)\n",
    "y_forecast_north = n_one_step_ahead_prediction(ada_boost_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(\"MSE: \"+str(mean_squared_error(y_north_val, y_forecast_north)))\n",
    "#x_axis = df.iloc[-n_forecast:].index.to_numpy()\n",
    "visualise_forecast_vs_true(x_axis,y_north_val,y_forecast_north,\"Ada Boost\", baseline_y=ydummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=60, max_features=\"auto\",max_depth=14).fit(X_north, y_north)\n",
    "y_forecast_north = n_one_step_ahead_prediction(rf_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(mean_squared_error(y_north_val, y_forecast_north))\n",
    "\n",
    "# Visualise this\n",
    "visualise_forecast_vs_true(x_axis, y_north_val, y_forecast_north, model_name=\"Random Forest\", baseline_y=ydummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results are bad, try again and it'll probably change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=100, alpha=1000,max_iter=500).fit(X_north, y_north)\n",
    "y_forecast_north = n_one_step_ahead_prediction(mlp_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(mean_squared_error(y_north_val, y_forecast_north))\n",
    "visualise_forecast_vs_true(x_axis, y_north_val, y_forecast_north, model_name=\"MLP\", baseline_y=ydummy_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from forecasting import n_one_step_ahead_prediction\n",
    "from visualisation import visualise_forecast_vs_true\n",
    "ridge_model = Ridge(alpha=1 / (2 * 0.0001)).fit(X_north, y_north)\n",
    "y_forecast_north = n_one_step_ahead_prediction(ridge_model, X_north_val, n_forecast, lagged_points, relevant_lag_indexes=[0,1,2,3,4,5,11])\n",
    "print(\"MSE: \"+str(mean_squared_error(y_north_val, y_forecast_north)))\n",
    "x_axis = df.iloc[-n_forecast:].index.to_numpy()\n",
    "visualise_forecast_vs_true(x_axis,y_north_val,y_forecast_north,\"Ridge Regression\", baseline_y=ydummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion stuff\n",
    "Lets re-train our model 12 times (for each month) so we can forecast for each month and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is NorthBound only, needs to be done for south bound aswell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving commented out for now, takes about 50 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from models import twleve_month_forecast\n",
    "\n",
    "test_scores={}\n",
    "val_month_scores={}\n",
    "val_week_scores={}\n",
    "\n",
    "#(test_scores[\"Baseline\"],val_month_scores[\"Baseline\"],val_week_scores[\"Baseline\"])=twleve_month_forecast(df_north,df_south,n_forecast,lag_range,\"Baseline\",dummy_mean)\n",
    "#(test_scores[\"Ridge Regression\"],val_month_scores[\"Ridge Regression\"],val_week_scores[\"Ridge Regression\"])=twleve_month_forecast(df_north,df_south,n_forecast,lag_range,\"Ridge Regression\",ridge_model)\n",
    "#(test_scores[\"kNN\"],val_month_scores[\"kNN\"],val_week_scores[\"kNN\"])=twleve_month_forecast(df_north,df_south,n_forecast,lag_range,\"kNN\",knn_model)\n",
    "#(test_scores[\"Ada Boost\"],val_month_scores[\"Ada Boost\"],val_week_scores[\"Ada Boost\"])=twleve_month_forecast(df_north,df_south,n_forecast,lag_range,\"Ada Boost\",ada_boost_model)\n",
    "#(test_scores[\"Neural Network\"],val_month_scores[\"Neural Network\"],val_week_scores[\"Neural Network\"])=twleve_month_forecast(df_north,df_south,n_forecast,lag_range,\"Neural Network\",mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_test_scores=pd.DataFrame(test_scores).to_csv(\"model_scores/df_test_scores.csv\",index=True)\n",
    "#df_val_month_scores=pd.DataFrame(val_month_scores).to_csv(\"model_scores/df_val_month_scores.csv\",index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation import forecast_plot\n",
    "\n",
    "df_test_scores=pd.read_csv(\"model_scores/df_test_scores.csv\", index_col=0)\n",
    "df_val_month_scores=pd.read_csv(\"model_scores/df_val_month_scores.csv\", index_col=0)\n",
    "df_val_week_scores=pd.read_csv(\"model_scores/df_val_week_scores.csv\", index_col=0)\n",
    "\n",
    "\n",
    "forecast_plot(df_test_scores,\"Validation Mean Square Errors\",\"Month\",log_scale=False)\n",
    "forecast_plot(df_val_month_scores,\"2018 Forecast Testing Mean Square Errors\",\"Month\",log_scale=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make a table to use as a direct comparison of the models\n",
    "print(\" average Validation score\")\n",
    "print(\"Baseline\")\n",
    "print(df_test_scores[\"Baseline\"].mean())\n",
    "print(\"Ridge Regression\")\n",
    "print(df_test_scores[\"Ridge Regression\"].mean())\n",
    "print(\"kNN\")\n",
    "\n",
    "print(df_test_scores[\"kNN\"].mean())\n",
    "print(\"Ada Boost\")\n",
    "\n",
    "print(df_test_scores[\"Ada Boost\"].mean())\n",
    "print(\"Neural Network\")\n",
    "print(df_test_scores[\"Neural Network\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import visualise_month_forecast,one_month_forecasting_data_prep\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "\n",
    "model_list = {\"Baseline\":dummy_mean,\n",
    "                \"Ridge Regression\":ridge_model,\n",
    "                \"kNN\":knn_model, \n",
    "                \"Ada Boost\":ada_boost_model,\n",
    "                \"Neural Net\":mlp_model}\n",
    "\n",
    "visualise_month_forecast(model_list,\"Dec\",df_north,df_south,n_forecast,lag_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea5f822aeab596314334ea1e62fe92926319a59f671f6f66e274027cdf6cea6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
